#[cfg(test)]
use crate::error::EventListenerError;
use crate::{config::EventListenerConfig, error::Result};
use backoff::{future::retry, ExponentialBackoff};
use std::{
    collections::VecDeque,
    sync::{
        atomic::{AtomicU64, Ordering},
        Arc,
    },
    time::{Duration, Instant},
};
use tokio::sync::{Mutex, RwLock};
use tracing::{debug, error, info, warn};

/// é‡è¯•ä»»åŠ¡
#[derive(Debug, Clone)]
pub struct RetryTask<T> {
    /// ä»»åŠ¡ID
    pub id: String,
    /// ä»»åŠ¡æ•°æ®
    pub data: T,
    /// åˆ›å»ºæ—¶é—´
    pub created_at: Instant,
    /// é‡è¯•æ¬¡æ•°
    pub retry_count: u32,
    /// æœ€å¤§é‡è¯•æ¬¡æ•°
    pub max_retries: u32,
    /// ä¸‹æ¬¡é‡è¯•æ—¶é—´
    pub next_retry_at: Instant,
    /// é”™è¯¯å†å²
    pub error_history: Vec<String>,
}

impl<T> RetryTask<T> {
    /// åˆ›å»ºæ–°çš„é‡è¯•ä»»åŠ¡
    pub fn new(id: String, data: T, max_retries: u32) -> Self {
        let now = Instant::now();
        Self {
            id,
            data,
            created_at: now,
            retry_count: 0,
            max_retries,
            next_retry_at: now,
            error_history: Vec::new(),
        }
    }

    /// è®°å½•å¤±è´¥å¹¶è®¡ç®—ä¸‹æ¬¡é‡è¯•æ—¶é—´
    pub fn record_failure(&mut self, error: String, base_delay: Duration) {
        self.retry_count += 1;
        self.error_history.push(error);

        // æŒ‡æ•°é€€é¿ï¼šbase_delay * 2^retry_count
        let delay = base_delay * (2_u32.pow(self.retry_count.min(10))); // é™åˆ¶æœ€å¤§æŒ‡æ•°
        self.next_retry_at = Instant::now() + delay;
    }

    /// æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
    pub fn can_retry(&self) -> bool {
        self.retry_count < self.max_retries && Instant::now() >= self.next_retry_at
    }

    /// æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
    pub fn is_exhausted(&self) -> bool {
        self.retry_count >= self.max_retries
    }

    /// è·å–ä»»åŠ¡å¹´é¾„
    pub fn age(&self) -> Duration {
        Instant::now() - self.created_at
    }
}

/// é‡è¯•ç®¡ç†å™¨
///
/// è´Ÿè´£:
/// - ç®¡ç†å¤±è´¥ä»»åŠ¡çš„é‡è¯•é˜Ÿåˆ—
/// - å®ç°æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
/// - æä¾›é‡è¯•ç»Ÿè®¡å’Œç›‘æ§
/// - å¤„ç†ä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†
pub struct RetryManager<T: Clone + Send + Sync + 'static> {
    config: Arc<EventListenerConfig>,

    // é‡è¯•é…ç½®
    max_retries: u32,
    base_delay: Duration,
    max_queue_size: usize,

    // é‡è¯•é˜Ÿåˆ—
    retry_queue: Arc<Mutex<VecDeque<RetryTask<T>>>>,

    // ç»Ÿè®¡ä¿¡æ¯
    tasks_added: Arc<AtomicU64>,
    tasks_retried: Arc<AtomicU64>,
    tasks_succeeded: Arc<AtomicU64>,
    tasks_failed: Arc<AtomicU64>,
    tasks_expired: Arc<AtomicU64>,

    // è¿è¡ŒçŠ¶æ€
    is_running: Arc<RwLock<bool>>,
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct RetryStats {
    pub queue_size: usize,
    pub tasks_added: u64,
    pub tasks_retried: u64,
    pub tasks_succeeded: u64,
    pub tasks_failed: u64,
    pub tasks_expired: u64,
    pub success_rate: f64,
    pub is_running: bool,
}

impl<T: Clone + Send + Sync + 'static> RetryManager<T> {
    /// åˆ›å»ºæ–°çš„é‡è¯•ç®¡ç†å™¨
    pub fn new(config: &EventListenerConfig) -> Self {
        let config = Arc::new(config.clone());

        Self {
            config: Arc::clone(&config),
            max_retries: config.listener.max_retries,
            base_delay: Duration::from_millis(config.listener.retry_delay_ms),
            max_queue_size: 1000, // é»˜è®¤æœ€å¤§é˜Ÿåˆ—å¤§å°
            retry_queue: Arc::new(Mutex::new(VecDeque::new())),
            tasks_added: Arc::new(AtomicU64::new(0)),
            tasks_retried: Arc::new(AtomicU64::new(0)),
            tasks_succeeded: Arc::new(AtomicU64::new(0)),
            tasks_failed: Arc::new(AtomicU64::new(0)),
            tasks_expired: Arc::new(AtomicU64::new(0)),
            is_running: Arc::new(RwLock::new(false)),
        }
    }

    /// å¯åŠ¨é‡è¯•ç®¡ç†å™¨
    pub async fn start<F, Fut>(&self, retry_handler: F) -> Result<()>
    where
        F: Fn(T) -> Fut + Send + Sync + 'static,
        Fut: std::future::Future<Output = Result<()>> + Send,
    {
        let mut is_running = self.is_running.write().await;
        if *is_running {
            warn!("é‡è¯•ç®¡ç†å™¨å·²åœ¨è¿è¡Œä¸­");
            return Ok(());
        }
        *is_running = true;
        drop(is_running);

        info!("ğŸ”„ å¯åŠ¨é‡è¯•ç®¡ç†å™¨");

        let manager = self.clone();
        let retry_handler = Arc::new(retry_handler);

        tokio::spawn(async move {
            manager.retry_loop(retry_handler).await;

            let mut is_running = manager.is_running.write().await;
            *is_running = false;

            info!("ğŸ”„ é‡è¯•ç®¡ç†å™¨å·²åœæ­¢");
        });

        Ok(())
    }

    /// åœæ­¢é‡è¯•ç®¡ç†å™¨
    pub async fn stop(&self) -> Result<()> {
        info!("ğŸ›‘ åœæ­¢é‡è¯•ç®¡ç†å™¨");
        let mut is_running = self.is_running.write().await;
        *is_running = false;
        Ok(())
    }

    /// æ·»åŠ ä»»åŠ¡åˆ°é‡è¯•é˜Ÿåˆ—
    pub async fn add_task(&self, id: String, data: T) -> Result<()> {
        let mut queue = self.retry_queue.lock().await;

        // æ£€æŸ¥é˜Ÿåˆ—å¤§å°
        if queue.len() >= self.max_queue_size {
            warn!("âš ï¸ é‡è¯•é˜Ÿåˆ—å·²æ»¡ï¼Œç§»é™¤æœ€æ—§çš„ä»»åŠ¡");
            queue.pop_front();
            self.tasks_expired.fetch_add(1, Ordering::Relaxed);
        }

        let task = RetryTask::new(id.clone(), data, self.max_retries);
        queue.push_back(task);

        self.tasks_added.fetch_add(1, Ordering::Relaxed);
        debug!("ğŸ“¥ æ·»åŠ ä»»åŠ¡åˆ°é‡è¯•é˜Ÿåˆ—: {}", id);

        Ok(())
    }

    /// é‡è¯•å¾ªç¯
    async fn retry_loop<F, Fut>(&self, retry_handler: Arc<F>)
    where
        F: Fn(T) -> Fut + Send + Sync + 'static,
        Fut: std::future::Future<Output = Result<()>> + Send,
    {
        let mut retry_interval = tokio::time::interval(Duration::from_millis(1000)); // æ¯ç§’æ£€æŸ¥ä¸€æ¬¡

        while *self.is_running.read().await {
            retry_interval.tick().await;

            // å¤„ç†é‡è¯•é˜Ÿåˆ—
            if let Err(e) = self.process_retry_queue(&retry_handler).await {
                error!("âŒ å¤„ç†é‡è¯•é˜Ÿåˆ—å¤±è´¥: {}", e);
            }

            // æ¸…ç†è¿‡æœŸä»»åŠ¡
            self.cleanup_expired_tasks().await;
        }
    }

    /// å¤„ç†é‡è¯•é˜Ÿåˆ—
    async fn process_retry_queue<F, Fut>(&self, retry_handler: &Arc<F>) -> Result<()>
    where
        F: Fn(T) -> Fut + Send + Sync + 'static,
        Fut: std::future::Future<Output = Result<()>> + Send,
    {
        let mut tasks_to_retry = Vec::new();
        let mut tasks_to_remove = Vec::new();

        // æ”¶é›†éœ€è¦é‡è¯•çš„ä»»åŠ¡
        {
            let queue = self.retry_queue.lock().await;
            let len = queue.len();

            for i in 0..len {
                if let Some(task) = queue.get(i) {
                    if task.can_retry() {
                        tasks_to_retry.push((i, task.clone()));
                    } else if task.is_exhausted() {
                        tasks_to_remove.push(i);
                    }
                }
            }
        }

        // å¤„ç†éœ€è¦ç§»é™¤çš„ä»»åŠ¡ï¼ˆä»åå¾€å‰ç§»é™¤ï¼Œé¿å…ç´¢å¼•å˜åŒ–ï¼‰
        if !tasks_to_remove.is_empty() {
            let mut queue = self.retry_queue.lock().await;
            for &index in tasks_to_remove.iter().rev() {
                if let Some(task) = queue.remove(index) {
                    warn!("âŒ ä»»åŠ¡é‡è¯•æ¬¡æ•°å·²ç”¨å®Œï¼Œç§»é™¤: {} (é‡è¯•{}æ¬¡)", task.id, task.retry_count);
                    self.tasks_failed.fetch_add(1, Ordering::Relaxed);
                }
            }
        }

        // æ‰§è¡Œé‡è¯•ä»»åŠ¡
        for (_queue_index, mut task) in tasks_to_retry {
            debug!("ğŸ”„ é‡è¯•ä»»åŠ¡: {} (ç¬¬{}æ¬¡)", task.id, task.retry_count + 1);

            let task_id = task.id.clone(); // å…‹éš†task.idç”¨äºåç»­ä½¿ç”¨

            match retry_handler(task.data.clone()).await {
                Ok(()) => {
                    // é‡è¯•æˆåŠŸï¼Œä»é˜Ÿåˆ—ä¸­ç§»é™¤
                    {
                        let mut queue = self.retry_queue.lock().await;
                        // ç”±äºä¹‹å‰å¯èƒ½æœ‰ç§»é™¤æ“ä½œï¼Œéœ€è¦é‡æ–°æŸ¥æ‰¾ä»»åŠ¡ä½ç½®
                        if let Some(pos) = queue.iter().position(|t| t.id == task_id) {
                            queue.remove(pos);
                        }
                    }

                    self.tasks_succeeded.fetch_add(1, Ordering::Relaxed);
                    info!("âœ… ä»»åŠ¡é‡è¯•æˆåŠŸ: {}", task_id);
                }
                Err(e) => {
                    // é‡è¯•å¤±è´¥ï¼Œæ›´æ–°ä»»åŠ¡çŠ¶æ€
                    task.record_failure(e.to_string(), self.base_delay);

                    {
                        let mut queue = self.retry_queue.lock().await;
                        if let Some(pos) = queue.iter().position(|t| t.id == task_id) {
                            if let Some(queue_task) = queue.get_mut(pos) {
                                *queue_task = task;
                            }
                        }
                    }

                    self.tasks_retried.fetch_add(1, Ordering::Relaxed);
                    warn!("âš ï¸ ä»»åŠ¡é‡è¯•å¤±è´¥: {} - {}", task_id, e);
                }
            }
        }

        Ok(())
    }

    /// æ¸…ç†è¿‡æœŸä»»åŠ¡
    async fn cleanup_expired_tasks(&self) {
        let max_age = Duration::from_secs(3600); // 1å°æ—¶åè¿‡æœŸ
        let mut expired_count = 0;

        {
            let mut queue = self.retry_queue.lock().await;
            let _initial_len = queue.len();

            queue.retain(|task| {
                if task.age() > max_age {
                    expired_count += 1;
                    false
                } else {
                    true
                }
            });

            if expired_count > 0 {
                info!("ğŸ—‘ï¸ æ¸…ç†äº† {} ä¸ªè¿‡æœŸä»»åŠ¡", expired_count);
            }
        }

        if expired_count > 0 {
            self.tasks_expired.fetch_add(expired_count, Ordering::Relaxed);
        }
    }

    /// ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥æ‰§è¡Œæ“ä½œ
    pub async fn retry_with_backoff<F, Fut, R>(&self, operation: F) -> Result<R>
    where
        F: Fn() -> Fut,
        Fut: std::future::Future<Output = Result<R>>,
    {
        let backoff = ExponentialBackoff {
            initial_interval: self.base_delay,
            max_interval: Duration::from_secs(300), // æœ€å¤§5åˆ†é’Ÿ
            multiplier: 2.0,
            max_elapsed_time: Some(Duration::from_secs(1800)), // æ€»å…±æœ€å¤š30åˆ†é’Ÿ
            ..Default::default()
        };

        retry(backoff, || async {
            match operation().await {
                Ok(result) => Ok(result),
                Err(e) => {
                    warn!("ğŸ”„ æ“ä½œå¤±è´¥ï¼Œå°†é‡è¯•: {}", e);
                    Err(backoff::Error::transient(e))
                }
            }
        })
        .await
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_stats(&self) -> RetryStats {
        let queue_size = {
            let queue = self.retry_queue.lock().await;
            queue.len()
        };

        let tasks_added = self.tasks_added.load(Ordering::Relaxed);
        let tasks_succeeded = self.tasks_succeeded.load(Ordering::Relaxed);
        let tasks_failed = self.tasks_failed.load(Ordering::Relaxed);
        let total_completed = tasks_succeeded + tasks_failed;

        let success_rate = if total_completed > 0 {
            tasks_succeeded as f64 / total_completed as f64
        } else {
            1.0
        };

        RetryStats {
            queue_size,
            tasks_added,
            tasks_retried: self.tasks_retried.load(Ordering::Relaxed),
            tasks_succeeded,
            tasks_failed,
            tasks_expired: self.tasks_expired.load(Ordering::Relaxed),
            success_rate,
            is_running: *self.is_running.read().await,
        }
    }

    /// æ¸…ç©ºé‡è¯•é˜Ÿåˆ—
    pub async fn clear_queue(&self) -> usize {
        let mut queue = self.retry_queue.lock().await;
        let count = queue.len();
        queue.clear();

        info!("ğŸ—‘ï¸ æ¸…ç©ºé‡è¯•é˜Ÿåˆ—ï¼Œç§»é™¤äº† {} ä¸ªä»»åŠ¡", count);
        count
    }

    /// è·å–é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    pub async fn get_queue_tasks(&self) -> Vec<String> {
        let queue = self.retry_queue.lock().await;
        queue
            .iter()
            .map(|task| {
                format!(
                    "ID: {}, é‡è¯•æ¬¡æ•°: {}/{}, å¹´é¾„: {:?}",
                    task.id,
                    task.retry_count,
                    task.max_retries,
                    task.age()
                )
            })
            .collect()
    }

    /// æ£€æŸ¥é‡è¯•ç®¡ç†å™¨æ˜¯å¦å¥åº·
    pub async fn is_healthy(&self) -> bool {
        let is_running = *self.is_running.read().await;
        let queue_size = {
            let queue = self.retry_queue.lock().await;
            queue.len()
        };

        // è¿è¡Œä¸­ä¸”é˜Ÿåˆ—æœªè¿‡è½½
        is_running && queue_size < self.max_queue_size
    }
}

impl<T: Clone + Send + Sync + 'static> Clone for RetryManager<T> {
    fn clone(&self) -> Self {
        Self {
            config: Arc::clone(&self.config),
            max_retries: self.max_retries,
            base_delay: self.base_delay,
            max_queue_size: self.max_queue_size,
            retry_queue: Arc::clone(&self.retry_queue),
            tasks_added: Arc::clone(&self.tasks_added),
            tasks_retried: Arc::clone(&self.tasks_retried),
            tasks_succeeded: Arc::clone(&self.tasks_succeeded),
            tasks_failed: Arc::clone(&self.tasks_failed),
            tasks_expired: Arc::clone(&self.tasks_expired),
            is_running: Arc::clone(&self.is_running),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::AtomicU32;

    fn create_test_config() -> EventListenerConfig {
        EventListenerConfig {
            solana: crate::config::settings::SolanaConfig {
                rpc_url: "https://api.devnet.solana.com".to_string(),
                ws_url: "wss://api.devnet.solana.com".to_string(),
                commitment: "confirmed".to_string(),
                program_ids: vec![solana_sdk::pubkey::Pubkey::new_unique()],
                private_key: None,
            },
            database: crate::config::settings::DatabaseConfig {
                uri: "mongodb://localhost:27017".to_string(),
                database_name: "test".to_string(),
                max_connections: 10,
                min_connections: 2,
            },
            listener: crate::config::settings::ListenerConfig {
                batch_size: 100,
                sync_interval_secs: 30,
                max_retries: 3,
                retry_delay_ms: 100,
                signature_cache_size: 10000,
                checkpoint_save_interval_secs: 60,
                backoff: crate::config::settings::BackoffConfig::default(),
                batch_write: crate::config::settings::BatchWriteConfig::default(),
            },
            monitoring: crate::config::settings::MonitoringConfig {
                metrics_interval_secs: 60,
                enable_performance_monitoring: true,
                health_check_interval_secs: 30,
            },
        }
    }

    #[test]
    fn test_retry_task_creation() {
        let task = RetryTask::new("test_task".to_string(), "test_data".to_string(), 3);

        assert_eq!(task.id, "test_task");
        assert_eq!(task.data, "test_data");
        assert_eq!(task.retry_count, 0);
        assert_eq!(task.max_retries, 3);
        assert!(task.can_retry());
        assert!(!task.is_exhausted());
    }

    #[test]
    fn test_retry_task_failure_recording() {
        let mut task = RetryTask::new("test_task".to_string(), "test_data".to_string(), 3);
        let base_delay = Duration::from_millis(100);

        // è®°å½•ç¬¬ä¸€æ¬¡å¤±è´¥
        task.record_failure("Error 1".to_string(), base_delay);
        assert_eq!(task.retry_count, 1);
        assert_eq!(task.error_history.len(), 1);
        assert!(!task.is_exhausted());

        // è®°å½•ç¬¬äºŒæ¬¡å¤±è´¥
        task.record_failure("Error 2".to_string(), base_delay);
        assert_eq!(task.retry_count, 2);
        assert_eq!(task.error_history.len(), 2);

        // è®°å½•ç¬¬ä¸‰æ¬¡å¤±è´¥
        task.record_failure("Error 3".to_string(), base_delay);
        assert_eq!(task.retry_count, 3);
        assert!(task.is_exhausted());
    }

    #[tokio::test]
    async fn test_retry_manager_creation() {
        let config = create_test_config();
        let manager: RetryManager<String> = RetryManager::new(&config);

        let stats = manager.get_stats().await;
        assert_eq!(stats.queue_size, 0);
        assert_eq!(stats.tasks_added, 0);
        assert!(!stats.is_running);
    }

    #[tokio::test]
    async fn test_add_task() {
        let config = create_test_config();
        let manager: RetryManager<String> = RetryManager::new(&config);

        manager
            .add_task("task1".to_string(), "data1".to_string())
            .await
            .unwrap();

        let stats = manager.get_stats().await;
        assert_eq!(stats.queue_size, 1);
        assert_eq!(stats.tasks_added, 1);
    }

    #[tokio::test]
    async fn test_retry_with_backoff_success() {
        let config = create_test_config();
        let manager: RetryManager<String> = RetryManager::new(&config);

        let result = manager.retry_with_backoff(|| async { Ok("success".to_string()) }).await;

        assert!(result.is_ok());
        assert_eq!(result.unwrap(), "success");
    }

    #[tokio::test]
    async fn test_retry_with_backoff_failure() {
        let config = create_test_config();
        let manager: RetryManager<String> = RetryManager::new(&config);

        let attempt_count = Arc::new(AtomicU32::new(0));
        let attempt_count_clone = Arc::clone(&attempt_count);

        let result = manager
            .retry_with_backoff(|| {
                let count = Arc::clone(&attempt_count_clone);
                async move {
                    let current = count.fetch_add(1, Ordering::Relaxed);
                    if current < 2 {
                        Err(EventListenerError::Unknown("Temporary failure".to_string()))
                    } else {
                        Ok("success after retries".to_string())
                    }
                }
            })
            .await;

        assert!(result.is_ok());
        assert_eq!(result.unwrap(), "success after retries");
        assert!(attempt_count.load(Ordering::Relaxed) >= 3); // è‡³å°‘é‡è¯•äº†2æ¬¡
    }

    #[tokio::test]
    async fn test_clear_queue() {
        let config = create_test_config();
        let manager: RetryManager<String> = RetryManager::new(&config);

        // æ·»åŠ ä¸€äº›ä»»åŠ¡
        manager
            .add_task("task1".to_string(), "data1".to_string())
            .await
            .unwrap();
        manager
            .add_task("task2".to_string(), "data2".to_string())
            .await
            .unwrap();

        let initial_stats = manager.get_stats().await;
        assert_eq!(initial_stats.queue_size, 2);

        // æ¸…ç©ºé˜Ÿåˆ—
        let cleared_count = manager.clear_queue().await;
        assert_eq!(cleared_count, 2);

        let final_stats = manager.get_stats().await;
        assert_eq!(final_stats.queue_size, 0);
    }

    #[tokio::test]
    async fn test_is_healthy() {
        let config = create_test_config();
        let manager: RetryManager<String> = RetryManager::new(&config);

        // åˆå§‹çŠ¶æ€ä¸å¥åº·ï¼ˆæœªè¿è¡Œï¼‰
        assert!(!manager.is_healthy().await);

        // æ¨¡æ‹Ÿè¿è¡ŒçŠ¶æ€
        {
            let mut is_running = manager.is_running.write().await;
            *is_running = true;
        }

        // è¿è¡ŒçŠ¶æ€ä¸‹åº”è¯¥å¥åº·
        assert!(manager.is_healthy().await);
    }
}
